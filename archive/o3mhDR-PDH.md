Got it! I’ll create a detailed PRD for your AI-generated TikTok-like app, ensuring it supports:

- **Plug-and-play AI content generation APIs** (allowing for flexibility in video, audio, and text sources like Runway, Sora, Pika Labs, etc.).
- **Short-form AI-generated videos (5-60 seconds)** as the primary format.
- **Pre-generated/batch-generated videos stored in Firebase Storage** rather than on-the-fly generation (to manage cost and feasibility).
- **User influence over AI-generated content through a suggestion mechanism** rather than direct prompt input, ensuring human oversight in incorporating feedback into future content batches.
- **Traditional engagement paradigms (likes, comments, follows)** rather than new experimental mechanisms.
- **A hybrid recommendation system** where users select interests on sign-up and a simple behavioral engine refines their feed over time.

The PRD will include **explicit API contracts, database schema, and feature requirements** to ensure modularity and ease of iteration. I’ll get back to you shortly with a detailed document.

# Product Requirements Document: AI-Generated Short-Form Video App

## Project Overview
This project is an AI-driven, TikTok-like mobile application focused on generating and delivering short-form video content tailored to each user’s interests. The goal is to automatically create a personalized feed of videos (5–60 seconds each) using generative AI models, aligning content with the preferences users provide. By leveraging multiple state-of-the-art AI video generation providers (such as Runway, OpenAI’s *Sora*, Pika Labs, etc.), the app can produce a diverse range of videos on-the-fly to match different themes and styles. A core principle of the project is a **modular architecture** – the system is designed so that AI content generation providers are plug-and-play components. This modularity ensures that as new and better generative models emerge (and indeed industry leaders anticipate “thousands of specialized models” excelling in various niches), the app can easily integrate or swap providers with minimal changes. Ultimately, the product aims to keep users engaged with an endless, on-demand feed of AI-generated videos that feel relevant and interesting to them, while also allowing user feedback to continually refine and improve the content.

## Features
- **Content Generation**: Supports multiple AI models for video generation (e.g. Runway Gen-2, OpenAI’s *Sora*, Pika Labs, etc.) to create short videos of 5–60 seconds. The system can generate videos across various categories (e.g. travel, music visuals, sports highlights, memes) based on prompts or themes. By using a flexible provider interface, the app can utilize different generative engines and even combine their outputs. Content is **pre-generated** (not in real-time during feed playback) to ensure smooth delivery to users.
- **User Feed & Recommendations**: Upon sign-up, users select their interests (e.g. categories or keywords that appeal to them). The app provides a personalized **For You** feed populated with AI videos matching those interests. Over time, a simple recommendation algorithm refines the feed based on user behavior – for instance, liking or watching certain topics more will cause similar content to appear more often. The feed updates continuously (in an infinite scroll interface) with fresh AI-generated clips, avoiding repeats and adjusting to the user’s evolving tastes.
- **User Engagement**: Standard social engagement features are included to enrich the experience. Users can **like** videos (indicating enjoyment), **comment** to discuss or react, and **follow** other users (or possibly curated AI content channels) to see content influenced by those follows. These interactions not only foster community but also provide signals to the recommendation system (e.g. a like signals interest in that content type). Engagement metrics (like counts, comments) are visible to users as they would be in a TikTok-style app.
- **Feedback-Driven Content Generation**: The platform encourages users to participate in the creative process by suggesting ideas for future videos. Users can submit requests or descriptions of what they’d like to see. These suggestions go into a feedback pipeline visible to administrators. With admin oversight, the best or most feasible ideas are approved and then fed into the content generation system, influencing upcoming batches of AI-created videos. This closes the loop between users and the AI — popular user ideas directly yield new content, keeping the catalog fresh and user-focused.
- **Video Storage & Delivery**: All AI-generated videos are stored in advance on a cloud storage (Firebase Cloud Storage) for fast and reliable delivery. Once a video is generated by the AI, it’s uploaded to storage and served to users from there (via CDN-backed URLs) rather than generating on-demand. This ensures that when a user scrolls their feed, videos play instantly with minimal buffering. Using Firebase Storage leverages Google’s scalable infrastructure to quickly serve media content to users around the world. This setup is optimized for performance and can handle high load, as videos are cached and delivered efficiently to the client.

## Requirements for Each Feature

### Content Generation
- **UI/UX**: *(Mostly a backend/administrative feature).* There is no direct user-facing UI for on-demand video generation in the app (users are consumers of content, not creators in this model). However, admin users will have a simple interface or dashboard to manage content generation. This could be as basic as a form to input prompts or select which AI model to use for a new batch of videos, and a list view to monitor generation status. Regular users indirectly experience this feature through the steady stream of new videos that appear in their feed. If a user suggestion is taken up (see Feedback feature), the user might receive a notification or see a note that their idea was used in a new video.
- **Backend Logic**: This is the heart of the app’s content pipeline. A Firebase Cloud Function (or a set of functions) will coordinate video generation tasks. The process works as follows:
  - The system triggers content generation either on a schedule (e.g. X videos per day in various categories) or on-demand when an admin approves user suggestions or initiates a batch.
  - For each requested video, the Cloud Function selects an AI provider to use. The selection could be random, round-robin, or based on the content type (for example, use *Runway* for realistic videos, *Pika Labs* for animated ones, etc.). Because the architecture is modular, adding a new provider means implementing its API integration and registering it in the selection logic – no core logic changes are needed to start generating videos with the new model.
  - The Cloud Function calls the chosen AI model’s API or SDK with the appropriate parameters (e.g. a text prompt describing the scene, any style settings, desired duration). Each provider has its own API endpoints and auth keys; these are stored securely (for example, in Cloud Functions environment config). The function may need to poll or wait for completion if generation is not instantaneous. (If an AI model’s processing time is long, we might handle the request asynchronously or use a task queue – but in all cases the generation happens in the cloud, not on the client.)
  - Upon generation completion, the resulting video file (e.g. an MP4) is saved to Firebase Storage under a designated path (e.g. `/videos/{videoId}.mp4`). The function then creates a new document in the Firestore **Videos** collection with metadata: reference to the storage file, associated interest tags/category, length, timestamp, the model used, and any other relevant info (such as which user suggestion prompted it, if applicable).
  - If the generation fails or the model returns an inappropriate result, the system gracefully handles it (error logged, no video saved, maybe an admin gets alerted). Basic validation is done on outputs as well (ensure video duration is within 5–60s limit, content is in line with guidelines, etc.).
  - Because we plan to integrate third-party AI services, network calls and event-driven functions are central here. We rely on Firebase Cloud Functions’ ability to call external APIs securely in a serverless environment. This keeps API keys hidden and allows heavy processing to occur off the user’s device.
- **Dependencies**: This feature depends on external AI video generation providers and cloud infrastructure:
  - Accounts/API access for each AI provider (Runway, Sora, Pika Labs, etc.) – these must be obtained and maintained. The system should be able to switch providers or update API endpoints without affecting the rest of the app (hence the need for a modular integration layer).
  - Sufficient quotas or credits on these AI platforms to generate videos at scale. Monitoring of usage and graceful degradation (or queueing) if limits are hit is required.
  - Cloud Functions environment configured with necessary libraries (if any SDKs are used) and sufficient timeout limits. (If video generation can take up to, say, 30 seconds or more, ensure the Cloud Function timeout is set appropriately or consider using Cloud Run for longer processes.)
  - Firebase Storage for saving the videos and Firestore for metadata (these are covered in the Video Storage feature as well).
  - **Modularity Consideration**: The code for content generation will be organized so that each AI provider integration is a separate module or service class. For example, a common interface like `VideoGenerator.generate(prompt, options) -> videoFile` will have implementations for Runway, Sora, Pika, etc. To add a new provider, one would implement this interface for the new service and add it to a factory/registry. The rest of the system (from admin triggering to saving file) remains unchanged. This ensures future enhancements or provider swaps (say replacing one model with a better one) can be done with minimal effort.

### User Feed & Recommendations
- **UI/UX**: The app opens to a **For You feed** – an infinite scroll of videos that autoplays one at a time (akin to TikTok’s full-screen vertical video experience). Key UI elements:
  - **Onboarding Interests Selection**: When a new user signs up, they are prompted with a list of topics or genres (displayed as toggles, chips, or a multi-select list). The user picks a few that appeal to them (e.g. “Comedy”, “Travel”, “Art”, etc.). This selection seeds their initial feed. The UI should make it easy to choose and maybe skip if they aren’t sure (with a default variety feed fallback).
  - **Video Player Feed**: Each video is shown in full, with the typical overlay: a like button, comment button, etc. Swiping up moves to the next video. The app preloads the next video in the background for seamless playback. Users can refresh or just keep scrolling for new content. There might be a subtle loading indicator or “Fetching more videos…” message if the app hits the end of preloaded content (in practice the feed is endless, cycling in new AI videos as they are created).
  - **Follow/Following Tabs** *(optional for later phase)*: If the “follow” feature is used to follow other users or specific content channels, the UI can offer a tab to switch between “For You” (AI recommended) and “Following” (videos influenced by followed users or channels). In an initial version, we might focus solely on the personalized feed and introduce a following feed once there are user-generated or curated streams to follow.
- **Recommendation Logic**: The feed is tailored using a straightforward algorithm that evolves with user input:
  - Initially, the app will query for videos that match the user’s selected interest categories. For example, if a user chose “Sports” and “Music”, the feed will fetch a mix of the latest or most relevant videos tagged in those categories. If multiple interests are selected, the feed can alternate between them or proportionally show more from the top-picked interest.
  - As the user interacts, the app refines what “relevant” means. **Positive signals** like watching a video to completion, pressing the like button, or watching it multiple times indicate the user enjoyed that content. **Negative signals** might include skipping the video quickly or long periods of inactivity on a video. Over time, the backend can adjust a user’s preference profile: for example, increasing the weight of “sports” if they consistently like sports clips, and reducing topics they consistently skip.
  - The recommendation system at this stage will be rule-based and transparent. For instance, it may maintain a simple score or affinity for each category per user. This score increases when they like or watch videos of that category, and maybe decays slowly if they ignore that category. The feed query then orders content by these personalized scores combined with recency (to ensure new content appears). We might also mix in some content outside their chosen interests occasionally, to test new interests (exploration) and avoid the feed becoming too narrow.
  - The app avoids showing the same video twice to a user. To support this, it keeps track of videos a user has already viewed (e.g. storing viewed video IDs in the user’s profile or caching on the client) and filters those out in queries.
  - The feed should also incorporate a freshness factor – for example, prefer newer videos (those generated recently) so the content feels up-to-date. Older content can still appear, but if the user has been very active, they might eventually see all relevant videos, at which point the system recirculates slightly older but still engaging ones (or encourages them to pick more interests).
- **Backend Implementation**: 
  - User interest selections at sign-up are saved in Firestore (e.g. in the user’s document under a field `interests: [ "Sports", "Music", ...]`). The list of possible interests (categories) can be a fixed set defined by the product team or stored in a separate collection for configurability.
  - To fetch the feed, the app (client) can call a **Cloud Function** (e.g. `getRecommendedFeed(userId)`) or perform a Firestore query directly. A simple approach is a Firestore query like: *“SELECT videos WHERE category IN user.interests ORDER BY createdAt DESC LIMIT 10”*. This grabs recent videos in the user’s interest areas. The client can then load more as needed by paginating (using Firestore cursors or timestamps).
    - To incorporate behavioral tuning (the scores for interests), a more advanced approach is for the Cloud Function to maintain a weighted list of interests or even specific video IDs for each user. For example, we could store in the user’s profile something like `prefScores: { "Sports": 15, "Music": 8, "Art": 3 }`. Then `getRecommendedFeed` could fetch a few videos from Sports for every one from Art, etc., reflecting the scores. In Firestore, a single query with multiple categories might not allow weighting easily, so the function might need to do multiple queries per category and merge results.
    - Given the relatively small scale of initial content, a practical method is: for each request, pick the top N categories of the user, query a couple of videos from each (that the user hasn't seen), then shuffle/merge them into one list. This can be done in a Cloud Function, which then returns the list to the client. The client will display them in order and request more when needed.
  - The system can use **Firebase Cloud Functions (scheduled)** to perform any periodic calculations, such as updating trending content or clearing out old data. For instance, to keep the feed fresh, a scheduled function might mark videos older than a certain age as less relevant or create a "trending" collection of currently popular videos (based on likes/views) to mix into feeds. (One suggested approach from the Firebase community is to periodically compute a ranking score for posts and use that for feed ordering, though our initial approach will be simpler).
  - The **Follow** feature (if users can follow others) would influence the feed by including content that those followed users enjoyed or any posts they make. Since in our app content is AI-generated, “following” might serve a different purpose – perhaps following a user means you want to see videos that user has liked or their suggestions when they get made into videos. Backend-wise, if we implement following, we would retrieve not only the AI-curated feed but also insert any videos that are trending among your follows. However, in an MVP, following might be reserved for social features outside of the core AI feed (and possibly a separate tab).
- **Dependencies**: 
  - Firestore queries and indexes: We will need composite indexes on the Videos collection for queries filtering by category and sorting by date or score. The data model must support efficient retrieval by interest tags.
  - Sufficient network performance to handle continuous video loading. Since videos are pre-stored, the main load is on bandwidth. We rely on Firebase/Google Cloud’s CDN to serve videos quickly to clients globally.
  - Cloud Functions for any server-side combination logic. Also, security rules to ensure users can only fetch appropriate data (though most content is public, we still protect writes).
  - Analytics tracking to verify that the feed algorithm is performing (e.g. measuring watch times, like rates per user to see if engagement increases with personalization).

### User Engagement (Likes, Comments, Follows)
- **UI/UX**: Each video in the feed comes with familiar interactive buttons:
  - **Like** – represented by a heart icon. Tapping it toggles the like status. When liked, the heart is filled, and the like count on the video increments (reflected in real-time via update from the backend). Users can double-tap the video itself as a shortcut to like (a common TikTok UX pattern).
  - **Comment** – represented by a speech bubble icon. Tapping opens a comments view, where the user can read others’ comments on the video and add their own. The comments view may slide up over the video or navigate to a separate screen. Each comment shows the commenter’s username, profile pic (if any), and the comment text and timestamp. Users can scroll through comments and perhaps **like** individual comments as well (optional).
  - **Follow** – on an AI-generated content platform, this button’s behavior depends on what “entity” can be followed. If we allow users to follow each other, typically a follow button would appear on the video poster’s profile. However, since videos are AI-generated and not owned by a specific user, one approach is to attribute videos to an "AI Creator" or simply have the follow button lead to the app’s suggestion of “follow this topic”. For MVP, we might repurpose the follow feature such that users can follow *other users* (social aspect) or follow *topics*. If it’s other users, a user’s profile page will have a Follow button. If it’s topics, the follow button could appear when viewing a category or a special account representing that category.
  - **Share** – (not explicitly listed, but typically present) an option to share the video link with others or on social media. This would generate a shareable link to view the video, likely via a web player or requiring the app.
  - Standard UI flows like confirming a like (animation on the heart), error messages if a comment fails to post, etc., should be in place.
- **Backend Logic**:
  - **Likes**: When a user likes a video, the app will update Firestore to record this engagement. This can be done in two steps:
    1. Create or update a record of the like. We can use a subcollection or separate collection, e.g. `Likes` where each document has a `userId` and `videoId`. Alternatively, we can maintain an array of `likedVideoIds` in the user’s document. For scalability, a separate `Likes` collection (or a `userLikes` subcollection under each user) is preferred so we don’t hit array size limits and can query relationships. A like operation will create a new Like document (if not already liked) or remove it if unliked.
    2. Update the aggregate like count on the corresponding video document. We maintain a `videos.likesCount` field. This increment can be done safely using Firestore’s atomic increment operation. For example, a Cloud Function trigger can listen for new Like documents and increment the count, or the client can directly call a Cloud Function `likeVideo(user, video)` that does both the write and increment in a transaction.
  - **Comments**: When a user posts a comment:
    - A new document in a `Comments` collection (or as a subcollection under the video) is created. Fields include: `videoId`, `userId`, `text`, `timestamp`. If we use a subcollection (e.g. `/videos/{videoId}/comments/{commentId}`), it’s easy to query comments for a specific video. We might also have a top-level `Comments` collection for all comments (useful if we need to moderate or analyze them globally).
    - Similar to likes, we update a `videos.commentsCount` field for the video. A trigger or transaction can increment the count on new comment.
    - Optionally, we can implement a simple notification: if comments need to notify someone (for instance, if in the future there are content creators or if users follow discussions), but for now, since content is AI-generated, notifications might be minimal.
    - We should implement basic profanity filtering or abuse reporting for comments, as user-generated text could be problematic. This could be an extension beyond PRD scope, but flagged for moderation.
  - **Follows**: If users can follow other users:
    - A `Follows` collection (or `following` subcollection under user) will store follow relationships. For example, a document with `followerId` and `followingId` indicates one user follows another. We may also maintain a `followersCount` on the user profile of the one being followed, and a `followingCount` on the follower’s profile.
    - Following doesn’t directly affect content (since content isn’t posted by users), but it sets the stage for social features. In the future, if users could create playlists of AI videos or their own generated content, followers would see those. For the recommendation engine, if we want to utilize follows, one idea is showing videos that many people you follow liked (a social recommendation).
    - The follow/unfollow action can be done via a Cloud Function `followUser(follower, target)` that writes the relationship and updates counts.
  - **Views/Watch tracking**: In addition to likes and comments, simply watching a video is a valuable engagement signal. We will track video views in a lightweight way:
    - Every time a user watches a video past a certain threshold (say 75% of it or at least N seconds), we count it as a view. We can update a `videos.viewCount` field for analytics and possibly for trending calculations.
    - We might not create a separate document for each view due to volume, but rather aggregate counts or mark in the user’s history that they watched it. The user’s document could have a map of `lastViewedAt: { [videoId]: timestamp }` or a list of `viewedVideoIds` (kept to a recent window).
    - This information is used by the recommendation logic to avoid repeats and measure popularity.
  - All these engagement operations must be **real-time** to the user if possible. Firestore’s real-time subscriptions can be leveraged so that when like counts or comments change, users see updated numbers without manual refresh.
- **Dependencies**:
  - Firestore data structure must support relational data efficiently. We will likely define indexes for queries like “all likes by a user” or “all comments on a video” if needed.
  - Security and rules: Only authenticated users can write likes/comments. Rules ensure one cannot like on behalf of another user or manipulate counts directly. (Use server functions for sensitive updates to keep logic secure).
  - Possibly integration with Firebase Authentication for user accounts, since engagement is tied to user identity. We need unique user IDs from Auth and basic profile info.
  - Cloud Functions triggers: as noted, for keeping counts in sync (like count, comment count) and possibly for sending notifications (e.g. “Your suggestion was turned into a video!” as a future feature).

### Feedback-Driven Content Generation (User Suggestions)
- **UI/UX**: To involve users in content creation, the app provides a **“Suggest a Video”** feature:
  - There will be an accessible option (perhaps on the user’s profile screen or a dedicated section in the app menu) for submitting video ideas. This could be a simple text input labeled e.g. “What would you like to see? Describe a video idea:”. The user enters a prompt or suggestion (for example, *“A cat surfing on a wave during sunset”*).
  - We may guide the user with some examples or structure (maybe ask for a category selection along with the description, to help route it). But free text gives flexibility for creative ideas.
  - After submission, the user gets a confirmation message like “Thanks for your suggestion! Our AI creators might use it soon.” We set expectations that not every suggestion will immediately become a video.
  - The suggestion UI might also show the status of their past suggestions (e.g. “pending review”, or “added to next batch”, or “now live!” with a link to the video if produced). This encourages transparency and continued engagement.
  - (Optional) If volume of suggestions is high, we might implement a voting or liking mechanism on suggestions in future, or limit how often each user can suggest (to maintain quality).
- **Admin & Moderation**: 
  - Suggestions go into an admin workflow. Admins will have a web dashboard (or even use Firebase Console in early stage) to review incoming suggestions. The dashboard lists suggestions with the text, user info, timestamp, and maybe upvote counts if we allow voting. 
  - Admin can filter out inappropriate or non-viable suggestions. For example, anything against content guidelines is rejected. Admin marks suggestions as **Approved** for those they want to turn into videos. They might also categorize or edit the suggestion for clarity before generation (e.g. if a user wrote a very long or vague prompt, the admin can tweak it).
  - Once approved, an admin can trigger the generation for that suggestion. The UI might allow selecting which AI model to use (if a particular model is better suited) or just use a default pipeline.
  - Batch processing: Admins might approve several suggestions and then hit “Generate Now” to process them in batch, or it could be automated daily – e.g. every day at 6pm, take all approved suggestions and create videos for them.
- **Backend Logic**:
  - User suggestions are stored in Firestore in a `Suggestions` collection. Each suggestion document contains:
    - `userId` (reference to the user who suggested),
    - `text` (the suggestion prompt or description),
    - `status` (`pending`, `approved`, `rejected`, `done` etc.),
    - `createdAt` timestamp,
    - optionally an `adminNote` or `reason` if rejected,
    - if generated, a `videoId` linking to the resulting video document.
  - A Cloud Function (or an admin tool invocation) handles moving an approved suggestion into content generation:
    - This function finds suggestions with status “approved” that are not yet processed, and for each, calls the **Content Generation** pipeline (essentially reusing the logic from the content generation feature). The suggestion text can serve as the prompt for the AI model. For instance, if a suggestion says "Space travel vlog in cartoon style", the function passes that prompt to an AI model that handles cartoon-style videos.
    - The resulting video is stored and a new video document is created in Firestore as usual. Then the suggestion document is updated with `status: "done"` and the generated `videoId`.
    - If needed, the user could be notified at this point (we could write a notification entry in a `Notifications` collection or use Firebase Cloud Messaging to ping the user’s device that “Your suggested video is ready!”).
  - We maintain admin oversight at all times – generation is not automatic directly from user input without review. This prevents misuse and ensures quality control.
  - Over time, if this feedback loop is successful, we might even train a model on popular suggestions or automate more of the review process. The PRD emphasizes initial manual admin control to curate content from suggestions.
- **Dependencies**:
  - Firestore for storing suggestions and their statuses.
  - Cloud Functions for the suggestion handling logic (especially the function that converts approved suggestions into videos by calling AI generation).
  - The same AI providers as content generation, since suggestions feed into that system. It’s essentially another entry point to the content pipeline.
  - Possibly moderation tools or services (we might integrate an API for content moderation to scan suggestion text for disallowed content).
  - Administrative interface: we might use Firebase Authentication to designate admin users and build a simple admin web app (could even be a protected route in a web version of the app or a separate admin React app connected to the same backend).
  - Notification service (Firebase Cloud Messaging) if we implement user notifications for when their suggestion is used.

### Video Storage & Delivery
- **Storage of Videos**: All videos generated by the system are saved in **Firebase Cloud Storage**, which is a Google Cloud Storage bucket accessible via Firebase. Each video file gets a unique identifier (matching the Firestore video document ID) and is stored at a known path (e.g. `videos/{videoId}.mp4`). Storing content in Cloud Storage allows the app to leverage Google’s infrastructure for fast delivery and scalability – it’s designed to efficiently store and serve large media like photos and videos. 
  - We will organize the storage bucket perhaps by categories or date for easier management (e.g. `videos/sports/video123.mp4` or `videos/2023/11/30/videoXYZ.mp4`), though this is more for human convenience since programmatically the ID is sufficient.
  - Metadata like content type, size, etc., are automatically handled by Storage. We ensure the videos are in a streaming-friendly format (likely MP4/H.264 for broad device compatibility).
- **Delivery to App**: When the client app needs to play a video, it will retrieve the download URL from Firebase Storage (via the Firebase SDK or from the video’s Firestore data which might contain a pre-computed URL or path). We have a few options:
  - We can store a long-lived public URL in the Firestore document for each video (Firebase Storage can create a public link or use the bucket’s public access if we enable it). This way the client just uses that URL in a video player component.
  - Alternatively, the client can use Firebase’s SDK to fetch a tokenized download URL on the fly. This might be a bit slower on first load, but we can cache it or even include it in the video doc at creation time.
  - The content is served over HTTPS and benefits from Google’s global edge cache. Users should experience quick load times. For a TikTok-like effect, we will preload the next video (the app can fetch the URL for the next video in advance and begin buffering it while the current video plays).
- **Performance Considerations**: Because videos can be sizable (several MB each), we need to ensure smooth streaming:
  - We might limit the maximum video resolution or bitrate to balance quality with loading speed (e.g. 720p or lower if targeting mobile). The AI models might output higher resolution, so we could downscale or transcode if needed using a Cloud Function (this could be an extension: after generation, run a conversion to ensure consistent format/size).
  - Using Firebase Storage means we don’t have to manage a separate CDN explicitly – it’s built on Google Cloud’s infrastructure which is optimized for content delivery. According to Firebase, it is designed to "quickly and easily store and serve user-generated content, such as photos and videos".
  - We will set up appropriate caching headers on the videos if possible so that if a video is viewed frequently by many users (i.e., it’s trending), it stays cached at edge locations.
  - The app should handle cases where a video file is temporarily unavailable or slow (maybe retry or skip with an error message).
- **Data Flow**:
  - The Firestore **Videos** document will contain the info needed to fetch the video from Storage (either a full URL or the path plus maybe a version token). When the feed function delivers a list of videos to the client, it includes these URLs/paths.
  - The client then uses a video player component to play the video stream. If using a direct URL, it streams from there; if using the Firebase SDK, it will get a download URL then stream.
  - We ensure that video content is only fetched (and thus costs bandwidth) when needed – i.e., when a user is about to watch it. Preloading should be smart (only one or two ahead).
- **Dependencies**:
  - Firebase Storage bucket (and associated security rules). Likely, we will allow read access to the videos for all users (since there’s no privacy concern with AI-generated content) – possibly by making the files publicly readable or by using Firebase Auth tokens. Easiest is to make them public or use storage tokens in the URLs, so the app doesn’t have to pass along user credentials for each fetch.
  - Cloud Function for video generation already places files into Storage, so that integration is built-in. We might also use a Cloud Function to generate thumbnails or preview images for each video (for use in UI like a grid view or if sharing links).
  - The frontend video player (e.g. using a native video component on iOS/Android or an HTML5 video element in a web context) must handle streaming. We may consider using an adaptive streaming format (HLS/DASH) in the future if needed for varying network conditions, but initially MP4 progressive download is acceptable given short video lengths.
  - Monitoring and analytics on delivery: we will watch metrics like download latency, buffering events, etc., to ensure the storage solution meets performance needs.

## Data Models
We will use **Firebase Firestore** (NoSQL document database) for storing structured data (users, video metadata, etc.), and Firebase Storage for the video files as described. Below is the schema outline for Firestore:

- **Users Collection** (`users/{userId}`): Stores user profiles and preferences.
  - **Fields**:
    - `username` – String. The user’s display name.
    - `email` – String. (If using Firebase Auth, this might be stored in Auth and optionally duplicated here if needed).
    - `interests` – Array of Strings. The list of interest tags or category IDs the user selected at onboarding (e.g. `["Sports","Music","Art"]`).
    - `likedVideos` – *Option 1:* Array of Video IDs that the user has liked. (This is simple but could grow large; we might use a separate Likes collection instead as noted.) *Option 2:* Not stored in user directly, and we use a separate `likes` mapping.
    - `following` – Array of User IDs this user is following. (Again, if following many users, a subcollection like `users/{userId}/following/{followedUserId}` might be more scalable. We can implement either approach.)
    - `followersCount` – Number. (Optional) How many followers this user has (if follow feature is used and we want to display it).
    - `createdAt` – Timestamp of account creation.
    - (Any additional profile info like avatar URL, bio, etc., could be here if we allow profile personalization in the future.)
  - **Indexes**: We might index fields like `username` for search if needed. For now, most queries on users are by ID (fetching one’s own profile or another’s when viewing profile or follow suggestions).

- **Videos Collection** (`videos/{videoId}`): Stores metadata for each AI-generated video.
  - **Fields**:
    - `storagePath` – String. The path or URL for the video file in Firebase Storage (e.g. `"videos/video123.mp4"` or a full `https://firebasestorage.googleapis.com/...` link).
    - `category` – String. The primary category/interest tag of the video (e.g. `"Sports"`). There might be multiple tags if a video fits several interests; we can also store an array of tags (e.g. `tags: ["Sports","Comedy"]`) for richer classification.
    - `description` – String. (Optional) A text description or title for the video. This could be auto-generated or derived from the prompt (e.g. the suggestion text or prompt used to make it). It might be shown in the UI subtly for context or used for search.
    - `modelUsed` – String. (Optional) Which AI model generated this video (e.g. `"Runway Gen-2"` or `"PikaV2"`). Mainly for internal analysis or display if we choose to.
    - `duration` – Number. Length of the video in seconds.
    - `likesCount` – Number. How many likes the video has garnered.
    - `commentsCount` – Number. Number of comments on the video.
    - `viewCount` – Number. (Optional initially) How many times the video has been viewed (could be an approximate count).
    - `createdAt` – Timestamp. When the video was generated/added to the platform.
    - `suggestionId` – String. (Optional) If this video was created from a user suggestion, store a reference to that suggestion’s ID for traceability.
    - `active` – Boolean. (Optional) Could mark if the video is active/approved for showing. (In case we remove or archive videos, we can toggle this off so the feed won’t pick them up.)
  - **Indexes**: We will index `category` and `createdAt` for queries (compound index on category+createdAt for sorted fetch by category). If we implement a custom recommendation ranking, we might also store a `rankScore` field and index that for ordering the feed.

- **Comments Collection** (`videos/{videoId}/comments/{commentId}` or a top-level `comments/{commentId}` with a videoId field):
  - Using a subcollection under each video is convenient for fetching comments of one video.
  - **Fields**:
    - `userId` – String. ID of the user who wrote the comment (could also store username for quick display to avoid extra lookup, denormalizing for convenience).
    - `text` – String. The content of the comment.
    - `timestamp` – Timestamp. When the comment was posted.
  - If top-level, also include `videoId` to know which video the comment belongs to.
  - **Indexes**: For subcollection approach, Firestore automatically can query within that video’s comments. For a top-level approach, we’d index `videoId` for quick retrieval of comments by video.

- **Likes Collection** (`likes/{likeId}` or e.g. `users/{userId}/likes/{videoId}`):
  - If we use a separate collection for likes:
    - **Fields**: `userId`, `videoId`, `timestamp` (when liked). The document ID could be a combination of user+video (to enforce uniqueness).
    - We might not query likes very often except to check “did user X like video Y” (which could be done by looking for that doc) or “how many likes on video Y” (which we store in video doc anyway).
    - If we want to list all videos a user liked (for a profile “Likes” tab), we could query this collection by userId with index on that.
  - If we embed likes in user document (less preferred for scale), we’d have a field on user with array of videoIds they liked. Simpler, but we must watch out for array size and needing to download the whole array to check one video.
  - We will likely implement the separate `likes` collection approach for flexibility.

- **Follows Collection** (`follows/{relationId}` or `users/{userId}/following/{followedUserId}`):
  - **Fields**: `followerId`, `followingId`, `createdAt`.
  - If a user follows another, we either create a doc in a top-level collection or a doc in the follower’s subcollection. Either is fine; a top-level might allow easier global queries (like “who are all the followers of X” by filtering `followingId == X`).
  - We’ll also update counts in the Users collection as mentioned (so a user profile can display number of followers and following without heavy queries).

- **Suggestions Collection** (`suggestions/{suggestionId}`):
  - **Fields**:
    - `userId` – String. Who made the suggestion (reference to Users).
    - `text` – String. The suggestion content describing the desired video.
    - `status` – String. One of `pending` (default when created), `approved`, `rejected`, `done`.
    - `createdAt` – Timestamp of submission.
    - `approvedAt` – Timestamp when an admin approved it (if approved).
    - `processedAt` – Timestamp when a video was generated (if done).
    - `videoId` – String. If status is `done`, this links to the Videos entry that was created from this suggestion.
    - (Optional) `rejectionReason` – String, if an admin rejects it and wants to record why (could be useful for internal metrics or communicating back to user gently).
    - (Optional) `categoryHint` – String. If the user or admin classified the suggestion into a category, we might store it to route to appropriate model.
  - **Indexes**: By `status` (e.g. query all pending suggestions for admin to review). Also by `userId` if we want to show users their suggestions and statuses.

- **Categories Collection** (optional static collection for interest categories):
  - If we want the list of possible interests to be dynamic or localized, we could have `categories/{categoryId}` docs with:
    - `name` – "Sports", "Music", etc.
    - `description` or other metadata (like an icon image URL).
    - We might not need to frequently query this if the categories are hardcoded in the app, but keeping it in Firestore allows updating or adding categories without app update.

- **Analytics/Engagement Collection** (optional):
  - We might have a collection for aggregated stats or trending info (for internal use). For example, a daily job could compute top 10 videos of the day and store in `trending/{date}` or something. This isn’t required from day one, but the data model can accommodate such additions easily due to the schema-less nature of Firestore.

Overall, the data model is centered on the **Videos** collection as the main content store, the **Users** collection for personalization info, and supporting collections for the relationships (likes, comments, follows, suggestions). We aim to keep the schema flexible for future changes – for instance, if later we allow user-uploaded videos alongside AI videos, we can add a field `origin: "AI"| "user"` in the Videos docs, and the rest of the system remains compatible.

## API Contract
The app will use Firebase as the backend, so many interactions are facilitated via Firebase SDKs (which abstract away REST calls). However, we define here the key APIs/Cloud Functions and their contracts that coordinate our system’s features. All server logic is built with **Firebase Cloud Functions** (Node.js), which can be invoked through Firebase’s client SDK (for example, using callable functions) or triggered via Firestore/Auth events.

### Cloud Function Endpoints
*(Note: These could be implemented as HTTPS callable functions, which the client calls by name, or as REST endpoints. For clarity, we describe them as function calls.)*

- **`generateVideosBatch()`** – *Purpose:* Generate a batch of AI videos (could be called by an admin or a scheduled trigger).
  - **Trigger**: Invoked manually by admin via an admin interface, or scheduled (e.g. daily).
  - **Input**: Optionally, parameters like `categories` (to focus generation on certain topics), `count` (how many videos to generate), or list of specific `prompts` to use. If called as part of suggestion processing, it might not need input because it will fetch approved suggestions.
  - **Process**: The function orchestrates content generation as described in **Content Generation** section. It may loop and call external AI APIs for each video, handle responses.
  - **Output**: Returns a result summary (e.g. how many videos successfully generated). On the client/admin side, this might simply show a success message or list of new content IDs.
  - **Errors**: If generation fails for any prompt, it logs the error and continues with others. It can return a list of failures if needed.

- **`submitSuggestion(userId, text)`** – *Purpose:* Allow a user to suggest a video idea.
  - **Trigger**: Called from the app when a user submits the suggestion form.
  - **Input**: `userId` (the authenticated user’s ID, but this can be derived from the auth context instead of being passed explicitly in a callable function), and `text` (the suggestion content).
  - **Process**: Writes a new document in `suggestions` collection with status “pending”. Might run the text through a quick validation or moderation check (e.g., ensure it’s not empty and doesn’t contain banned words).
  - **Output**: Success/failure response. On success, returns the `suggestionId` or a confirmation message. On failure (e.g. text too long or offensive), returns an error message that the UI can show.

- **`approveSuggestion(suggestionId)`** – *Purpose:* Admin action to approve and generate a suggested video.
  - **Trigger**: Invoked by admin (likely through an admin dashboard when they click “approve” on a suggestion).
  - **Input**: `suggestionId` to process. Optionally, admin might specify a `model` or tweak the prompt, but that could be done separately.
  - **Process**: Marks the suggestion as approved in Firestore, then immediately (or via another function) triggers the creation of a video for it. We might integrate this with `generateVideosBatch` logic: e.g., `approveSuggestion` could simply set status=approved and then a scheduled function picks it up, or `approveSuggestion` could directly call the generation logic for that single prompt. Either way, it results in a new video in the Videos collection if successful.
  - **Output**: Returns status of the operation (e.g. “queued” or “generated”) and possibly the new `videoId` if immediately available. The admin dashboard can then show that the suggestion was processed.

- **`getRecommendedFeed(userId)`** – *Purpose:* Fetch a batch of videos for a user’s personalized feed.
  - **Trigger**: Called by the app when loading the feed (and for pagination when user scrolls).
  - **Input**: `userId` (again from auth context ideally), and maybe a `pageToken` or `lastVideoId` for pagination.
  - **Process**: This function implements the recommendation logic. It reads the user’s `interests` and possibly their engagement history (like their preference scores). It then queries Firestore for relevant videos. It could also incorporate any additional logic (e.g. exclude videos the user has seen by checking a list in user profile).
  - **Output**: A list of video items (each item might include video metadata and a generated Firebase Storage download URL if needed). For example, `{ videos: [ {videoId, storagePath, category, duration, likesCount, ...}, ... ], nextPageToken: ... }`. The `nextPageToken` could be a timestamp or document ID to use for fetching the next set (for pagination).
  - **Security**: Ensure that this function only returns appropriate content. (All content is generally public in this app, but if we had age-restricted categories, we’d filter here based on user).

- **`likeVideo(userId, videoId, isLike)`** – *Purpose:* Handle user liking or unliking a video.
  - **Trigger**: Called when user taps like or unlike.
  - **Input**: `userId` (implicit from auth), `videoId`, and `isLike` (boolean, true if liking, false if unliking).
  - **Process**: If `isLike=true`, it creates a Like record (if one doesn’t exist) and increments the video’s likesCount. If `isLike=false`, it removes the Like record and decrements the count. This should be done atomically or with a transaction to avoid race conditions (e.g., two quick like/unlikes).
  - **Output**: Returns success or failure. If success, possibly return the new like count (though the client could also observe this via real-time updates).
  - We might implement the like as a Firestore direct write (client adds a doc to `likes` and security rules allow it if userId matches, etc., with a trigger function updating count). Either approach is fine; the explicit API via Cloud Function gives more control and security.

- **`postComment(userId, videoId, text)`** – *Purpose:* Add a comment to a video.
  - **Trigger**: Called when user submits a comment.
  - **Input**: `userId` (auth), `videoId`, `text` (comment content).
  - **Process**: Creates a new comment document in the appropriate place. Could also check for profanity or spam. Then increments the video’s commentsCount. Possibly could also return the comment with an ID for immediate UI display.
  - **Output**: Success or error message. On success, the comment is persisted and the updated comment count could be returned.

- **`followUser(followerId, targetUserId, isFollow)`** – *Purpose:* Follow or unfollow another user.
  - **Trigger**: Called when a user taps follow/unfollow on someone’s profile.
  - **Input**: `followerId` (implicit from auth), `targetUserId`, and `isFollow` (true for follow, false for unfollow).
  - **Process**: If follow, create a follow document (or entry in follower’s subcollection) and increment the target user’s followersCount. If unfollow, remove that document and decrement count.
  - **Output**: Success/failure. On success, maybe return updated follow counts.

- *(Additionally, Firebase Authentication provides endpoints for login, signup, etc., which are handled by Firebase – not custom-built. We will use Firebase Auth UI or SDK for user management. Firestore security rules serve as the authorization layer for the above operations along with these functions.)*

### Integration with AI Providers
The Cloud Functions responsible for content generation (`generateVideosBatch`, `approveSuggestion` processes) will call external APIs. Each provider (Runway, Sora, Pika) has its own API contract:
- For example, Runway Gen-2 might expect an HTTP POST with a JSON body containing a text prompt and returns a URL to a generated video or a job ID for polling. We would code the function to handle that interaction.
- We might build a **provider service module** for each:
  - `runwayService.generateVideo(prompt)`,
  - `soraService.generateVideo(prompt)`, etc., which encapsulate the HTTP requests and handle responses.
- These would use libraries like Axios or the provider’s SDK within the Cloud Function. Because Cloud Functions can perform HTTP(S) calls to third-party services, we can integrate any RESTful or GraphQL API provided by these AI platforms.
- API keys or auth tokens for these services will be stored in Cloud Function configuration (not in code) for security. Each call will include the necessary auth header.
- The modular architecture means the main app doesn’t need to know details of each API – just that a video for a given prompt can be generated. If one provider is slow or down, we could fallback to another, or if we want to experiment with a new provider, we add it and adjust a config to start using it.

### Firestore and Cloud Storage Access Patterns
- The client app will mostly use the cloud functions above and Firestore queries to get data. In some cases, the client might read Firestore directly:
  - E.g. after calling `getRecommendedFeed`, it could directly listen to those video documents for real-time updates (like to update like counts live if someone else likes it – though in a social network of AI content, that real-time aspect is less critical than in a fully user-generated content scenario).
  - When viewing comments, the client might open a listener on `videos/{id}/comments` to get live comment updates as others comment.
- We will use Firebase’s offline cache capabilities for Firestore to make the feed smooth (so even if network flickers, the user can still scroll already fetched content).
- The API contract implies certain **Firestore rules**: for instance, normal users cannot write directly to `videos` or `users` aside from their own profile; suggestions can be written by users but only certain fields; likes can only be created by the liking user, etc. We will define rules such that:
  - `users/{uid}` can be written only by that `uid` (and only certain fields).
  - `videos` probably read-only to clients (only admins or cloud functions write).
  - `comments` can be created by authenticated users (maybe allow write to `videos/*/comments` with field validation).
  - Using Cloud Functions for things like likes/follows simplifies security because the function runs with privileges and can update multiple spots. The client then only calls the function, not directly writing multi-step updates.

### Future Enhancements (API Flexibility)
The chosen approach sets us up to extend the API easily:
- If we later add **search functionality** (to search videos by keyword), we could add a function `searchVideos(query)` or use an integrated Algolia/Elastic. The data model (with descriptions and tags) supports that.
- If we integrate a new AI provider, we just add its implementation and possibly an endpoint to utilize it (though likely it fits into existing generateVideo function logic).
- If we allow **content moderation or filtering**, we might add an admin API to remove a video: e.g. `removeVideo(videoId)` that sets a flag or deletes the video doc and file (and our feed function would skip any with `active=false`).
- The modular design, especially for content generation, ensures that the **API contracts with external services** are abstracted. For instance, whether a video came from Runway’s API or Pika’s, the resulting saved video and Firestore entry look the same to the rest of the app. This isolation means we can update or change providers without impacting the feed or user engagement features.

In summary, this PRD outlines a system where each component – content generation, feed, engagement, feedback loop, and delivery – is defined with clear responsibilities and interfaces. The use of Firebase’s ecosystem (Firestore, Cloud Functions, Storage, Auth) provides a robust backbone that is scalable from an MVP to a larger user base. Crucially, the emphasis on **modularity** (especially in the AI generation module) future-proofs the app in the fast-evolving landscape of AI video generation, allowing it to adopt new technologies and enhancements with minimal friction. Each feature and API has been specified to ensure the product delivers a seamless, personalized short-video experience powered by generative AI.